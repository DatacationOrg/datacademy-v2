{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacademy.modules import Module07\n",
    "\n",
    "module = Module07()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/Metro_Interstate_Traffic_Volume.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## A. Data Understanding\n",
    "First we will get acquainted with the data, for which you have to follow the steps outlined in `Easy-LMS`. In between steps we allow you to validate the shape of your data frame, which enables you to check whether you executed the previous steps correctly. To do this, simply pass the `list(df.shape)` into the checker function, for which the code will be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Investigate the first rows of the data frame using .head()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Analyse the numerical values of the data frame using .describe()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Extract column names for the numerical and categorical columns.\n",
    "numerical_columns = ...\n",
    "categorical_columns = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Generate a pairplot using the Seaborn library.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove the outliers in the 'temp' and 'rain_1h' columns.\n",
    "df = df[df['temp'] > ...]\n",
    "df = df[df['rain_1h'] < ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1 - Validate Data Frame\n",
    "Submit the shape of the data frame in the checker function below. Make sure to pass it as a list, using `list(df.shape)`, as our checker function is build to only work with lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Evaluate the shape of the data frame with the checker function below.\n",
    "module.check(\"E2_A1\", list(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Again generate a pairplot using the Seaborn library to see the difference.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print a box plot for all the numerical columns in the data frame.\n",
    "for num_col in numerical_columns:\n",
    "    plt.subplots()\n",
    "    plt.boxplot(x=...)\n",
    "    plt.title(f\"Boxplot: {num_col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print all categorical columns of the data frame.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the number of unique values in all category columns.\n",
    "for col in categorical_columns:\n",
    "    print(f\"{...} --> {...}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the unique values in the 'holiday' column using the .unique() function of Pandas.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## B. Data Preparation\n",
    "Now we have an understanding of our data, we can continue with preparing our data. The steps to do this are outlined in `Easy-LMS`, so follow these accordingly. In between you can check your data frame shapes in a similar manner as before, to validate if you executed the steps correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Run the script below to transform the 'date_time' column into a useable format.\n",
    "df['date_time'] = pd.to_datetime(df['date_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "df['date'] = df['date_time'].dt.date\n",
    "df['time'] = df['date_time'].dt.time\n",
    "\n",
    "df['DoW'] = df['date'].apply(lambda x: x.weekday())\n",
    "df['HoD'] = df['time'].apply(lambda x: x.hour)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1 | B2 - Validate Data Frame\n",
    "Run the check functions below to check whether the `Day of Week (DoW)` and `Hour of Day (HoD)` columns are constructed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the checker function to validate the 'DoW' column.\n",
    "module.check(\"E2_B1\", dict(Counter(df['DoW'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the checker function to validate the 'HoD' column.\n",
    "module.check(\"E2_B2\", dict(Counter(df['HoD'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a column names 'is_holiday' which contains a 1 if the row concerns a holiday.\n",
    "df['is_holiday'] = df['holiday'].notnull().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B3 - Validate Data Frame\n",
    "Run the check functions below to check whether the `is_holiday` column is constructed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the checker function to validate the 'is_holiday' column.\n",
    "module.check(\"E2_B3\", dict(Counter(df['is_holiday'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the unique holidays for both the 'is_holiday' equal to 1 and 0 separately.\n",
    "print(...)\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove the columns: 'holiday', 'date_time', 'date', 'time' and 'weather_description'.\n",
    "df.drop(...)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Split the data frame in independent (X) and dependent (y) data sets.\n",
    "X = ...\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a train-test split.\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B4 - Validate Train and Test data.\n",
    "Run the check functions below to check whether the `X_train`, `X_test`, `y_train` and `y_test` data sets are constructed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Validate the created train and test set using the checker function.\n",
    "module.check(\"E2_B4\", [X_train.shape, X_test.shape, y_train.shape, y_test.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Again extract column names for the numerical and categorical columns to include changes made.\n",
    "numerical_columns = ...\n",
    "categorical_columns = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Split the categorical and numerical columns in both the train and test set.\n",
    "cat_cols_train = X_train[categorical_columns]\n",
    "num_cols_train = X_train[numerical_columns]\n",
    "\n",
    "cat_cols_test = X_test[categorical_columns]\n",
    "num_cols_test = X_test[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Apply Min-Max Scaling to the categorical columns, prevent data leakage by only fitting on the train data.\n",
    "scl = ...\n",
    "\n",
    "scaled_train = ...\n",
    "\n",
    "scaled_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B5 - Validate Scaled Data.\n",
    "Run the check functions below to check whether the `scaled_train` and `scaled_test` data sets are constructed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Send the statistics of both the train and test set to evaluate their validity.\n",
    "train_stats = {\n",
    "    \"min\": min(scaled_train.min(axis=0)),\n",
    "    \"avg\": np.mean(np.mean(scaled_train, axis=0)),\n",
    "    \"max\": max(scaled_train.max(axis=0))\n",
    "}\n",
    "\n",
    "test_stats = {\n",
    "    \"min\": min(scaled_test.min(axis=0)),\n",
    "    \"avg\": np.mean(np.mean(scaled_test, axis=0)),\n",
    "    \"max\": max(scaled_test.max(axis=0))\n",
    "}\n",
    "\n",
    "module.check(\"E2_B5\", [train_stats, test_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Apply onehot-encoding to the categorical columns, again prevent data leakage by only fitting on the train data.\n",
    "enc = ...\n",
    "\n",
    "OH_train = ...\n",
    "\n",
    "OH_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B6 - Validate Encoded Data.\n",
    "Run the check functions below to check whether the `OH_train` and `OH_test` data sets are constructed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Run the checker function below to evaluate if the encoding is done correctly.\n",
    "module.check(\"E2_B6\", [int(np.sum(OH_train)), int(np.sum(OH_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Recombine the numerical and categorical columns of both the train and test set.\n",
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Retrieve the column names and created a prepared_df to visualize the data.\n",
    "column_names = ...\n",
    "prepared_df = pd.DataFrame(data={column_names[i]: X_train[:,i] for i in range(len(column_names))})\n",
    "prepared_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## C. Modeling and Evaluation\n",
    "Enough of the data preprocessing, it is time to develop and train some models! We will use the library `Scikit-Learn` to do so. This library allows the user to easily switch between models, as all models have a `.fit()` and `.predict()` function. Please make sure that during initialisation (if possible) you set:\n",
    "* `n_jobs` = -1, to increase speed through `parallel computation`. <br>\n",
    "* `random_state` = 0, to fixate the end result. <br>\n",
    "\n",
    "By following the steps outlined in `Easy-LMS` we will eventually develop three models, namely: \n",
    "* `Linear Regression`; <br>\n",
    "* `Random Forest Regressor`; <br>\n",
    "* `Supper Vector Regressor (SVR)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill the list below with all models | Make sure to add n_jobs and random_state where possible (hint: Look at documentation)\n",
    "models = [\n",
    "    LinearRegression(n_jobs=-1),\n",
    "    SVR(),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=0)\n",
    "]\n",
    "\n",
    "#TODO: Loop over the list of models and evaluate their performance using the r2-score.\n",
    "for model in models:\n",
    "    model.fit(...)\n",
    "    y_pred = ...\n",
    "    performance = ...\n",
    "    print(f'r2_score {type(model).__name__}: {round(performance, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C1 - Validate R2 Scores.\n",
    "Fill in the `r2 scores` that you printed above in the check functions below. It will validate whether the performance of the trained models correspond with our answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill the dictionary below and evaluate if your models are configured and trained correctly.\n",
    "r2_scores = {\n",
    "    \"LinearRegression\": ...,\n",
    "    \"SVR\": ...,\n",
    "    \"RandomForestRegressor\": ...\n",
    "    }\n",
    "\n",
    "module.check(\"E2_C1\", r2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## D. Modeling and Evaluation (Advanced)\n",
    "The process we just executed works, but is prone to potential `biases` in the data. For example, a favorable training set could increase the r2 score significantly. <br>\n",
    "\n",
    "To prevent this from happening we will implement `k-fold Cross Validation`, which trains the model on multiple train/test splits and returns an average result. However, as our process contained a lot of manual steps, executing these different splits by hand would be a cumbersome task. <br>\n",
    "\n",
    "For this reason we will replace our manual steps with a so-called `Pipeline`, which we construct using `Scikit-Learn`. This will enable us to evaluate the models using the `cross-validate()` function, also from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Finalize the function below to return a pipeline containing preprocessing steps and a regression model.\n",
    "def create_pipeline(scl, enc, reg) -> Pipeline:\n",
    "    \"\"\"Take the scaler, encoder and regressor and create and return a sklearn pipeline.\n",
    "\n",
    "    Args:\n",
    "        scl (_type_): Scaling module, used to scale the numerical data to a set range of values.\n",
    "        enc (_type_): Encoding module, used to transform categorical values to a workable format.\n",
    "        reg (_type_): Regression model, which can be any model from the sklearn regression model catalog.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: Pipeline containing all needed preprocessing steps and Regression model.\n",
    "    \"\"\"\n",
    "    numerical_pipe = ...\n",
    "    categorical_pipe = ...\n",
    "\n",
    "    preprocessor = ...\n",
    "\n",
    "    return Pipeline(steps=[(\"...\", ...), (\"...\", ...)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create an example pipeline and visualize it.\n",
    "pipe = create_pipeline(\n",
    "    scl=MinMaxScaler(),\n",
    "    enc=OneHotEncoder(),\n",
    "    reg=RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    ")\n",
    "\n",
    "#TODO: Apply 5-fold cross validaation on the created pipeline.\n",
    "cv_results = cross_validate(\n",
    "    estimator=..., \n",
    "    X=..., y=..., \n",
    "    cv=..., scoring='...', \n",
    "    n_jobs=...\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the model performance using the code below.\n",
    "print(f\"Performance of different folds: {[round(score, 2) for score in cv_results['test_score']]}\")\n",
    "print(f\"Average performance: {round(np.mean(cv_results['test_score']), 2)}\")\n",
    "print(f\"Standerd deviation: {round(np.std(cv_results['test_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D1 - Validate 5-fold Cross Validation Scores.\n",
    "Fill in the scores below to check whether your `cross_validate()` run returns similar values as our algorithms did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill the dictionary below and validate if you executed cross validation succesfully.\n",
    "cross_validate_scores = {\n",
    "    \"Fold_Performance\": [..., ..., ..., ..., ...],\n",
    "    \"Average_Performance\": ...,\n",
    "    \"Standard_Deviation\": ...\n",
    "}\n",
    "\n",
    "module.check(\"E2_D1\", cross_validate_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## E. Hyperparameter Optimization:\n",
    "Through elaborate testing of different models on our data set we define which model best suits the underlying data set. Normally these comparisons are done with the `hyperparameters` set to the `default` values. However, when we intend to put a model into operation, we want a model that is optimized towards the application. This can be attained through `hyperparameter optimization`, which we will conduct using `Grid Search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Develop parameter dictionary, create pipeline and apply Cross Validated Grid Search\n",
    "parameters = {\n",
    "    \"reg__n_estimators\": [...], \n",
    "    \"reg__max_depth\": [...]\n",
    "    }\n",
    "\n",
    "pipe = create_pipeline(\n",
    "    scl=MinMaxScaler(),\n",
    "    enc=OneHotEncoder(),\n",
    "    reg=RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=..., \n",
    "    param_grid=..., \n",
    "    cv=..., scoring=\"...\", \n",
    "    n_jobs=...\n",
    "    )\n",
    "\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the keys of the resulting dictionary using '.keys()'\n",
    "grid_search.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a Pandas data frame with the grid search performance.\n",
    "gridsearch_results = pd.DataFrame(\n",
    "    data={\n",
    "        \"max_depth\": grid_search.cv_results_[\"param_reg__max_depth\"],\n",
    "        \"n_estimators\": grid_search.cv_results_[\"param_reg__n_estimators\"],\n",
    "        \"mean_test_score\": grid_search.cv_results_[\"mean_test_score\"],\n",
    "        \"std_test_scores\": grid_search.cv_results_[\"std_test_score\"],\n",
    "        \"mean_fit_time\": grid_search.cv_results_[\"mean_fit_time\"],\n",
    "        \"mean_score_time\": grid_search.cv_results_[\"mean_score_time\"],\n",
    "    }\n",
    ")\n",
    "gridsearch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1 - Validate Grid Search Results.\n",
    "Fill in the requested values of the best performing configuration based on `mean_test_score` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill in the dictionary below and evaluate if you executed grid search succesfully.\n",
    "best_performing_configuration = {\n",
    "    \"max_depth\" : None,\n",
    "    \"n_estimators\" : 1000\n",
    "}\n",
    "\n",
    "module.check(\"E2_E1\", best_performing_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fit a new pipeline using the optimized parameter settings.\n",
    "optimized_pipe = create_pipeline(\n",
    "    scl=MinMaxScaler(),\n",
    "    enc=OneHotEncoder(),\n",
    "    reg=RandomForestRegressor(\n",
    "        n_estimators=1000, max_depth=None, random_state=0, n_jobs=-1\n",
    "    )\n",
    ").fit(X, y)\n",
    "\n",
    "optimized_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the function below to print the feature importances of our model.\n",
    "def plot_feature_importance(feature_importance:np.array, feature_names:np.array) -> None:\n",
    "\n",
    "    data={\n",
    "        \"feature_names\": feature_names,\n",
    "        \"feature_importance\": feature_importance\n",
    "        }\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    fi_df.sort_values(by=[\"feature_importance\"], ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure()\n",
    "    sns.barplot(x=fi_df[\"feature_importance\"], y=fi_df[\"feature_names\"])\n",
    "    plt.title(\"Random Forest Regressor - Feature Importances\")\n",
    "    plt.xlabel(\"Feature Importances\")\n",
    "    plt.ylabel(\"Feature Name\")\n",
    "\n",
    "plot_feature_importance(\n",
    "    feature_importance=optimized_pipe[\"reg\"].feature_importances_,\n",
    "    feature_names=column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the actual numerical values for the feature importances.\n",
    "feature_importances = optimized_pipe['reg'].feature_importances_\n",
    "for i in range(len(feature_importances)):\n",
    "    print(f\"{column_names[i]} --> {feature_importances[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
