{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import cross_validate, train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as imb_Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacademy.modules import Module07\n",
    "\n",
    "module = Module07()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/adult.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## A. Data Understanding\n",
    "First we will get acquainted with the data, for which you have to follow the steps outlined in `Easy-LMS`. In between steps we allow you to validate the shape of your data frame, which enables you to check whether you executed the previous steps correctly. To do this, simply pass the `list(df.shape)` into the checker function, for which the code will be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Investigate the first rows of the data frame using .head()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Analyse the numerical values of the data frame using .describe()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Analyse the different columns of the data frame using .info()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Generate a pairplot using the Seaborn library.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print a box plot for both the 'capital gain' and 'capital-loss' columns.\n",
    "for num_col in ['capital-gain', 'capital-loss']:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove the outliers in the 'capital-gain' and 'capital-loss' columns.\n",
    "print(df.shape)\n",
    "df = df[df['capital-gain'] < ...]\n",
    "df = df[df['capital-loss'] < ...]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1 - Validate Data Frame\n",
    "Submit the shape of the data frame in the checker function below. Make sure to pass it as a list, using `list(df.shape)`, as our checker function is build to only work with lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Submit the data frame shape to analyse if the outlier removal step is executed correctly.\n",
    "module.check(\"E3_A1\", list(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Extract column names for the numerical and categorical columns.\n",
    "numerical_columns = ...\n",
    "categorical_columns = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Loop through the categorical columns and print the number of unique categories.\n",
    "for col in categorical_columns:\n",
    "    print(f'{...}: {...}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the amount of unique values in the 'education_num' column.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the first 10 rows of the 'education_num' en 'education' columns.\n",
    "df[['...', '...']].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## B. Data Preparation\n",
    "Now we have an understanding of our data, we can continue with preparing our data. The steps to do this are outlined in `Easy-LMS`, so follow these accordingly. In between you can check your data frame shapes in a similar manner as before, to validate if you executed the steps correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove the 'education_num' and 'fnlwgt' columns and print first 5 rows.\n",
    "df.drop(...)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Transform the categorical values of the 'sex' column into binary values (0/1).\n",
    "genders = list(df['sex'].unique())\n",
    "df['sex'] = [genders.index(x) for x in df['sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Transform the categorical values of the 'class' column into binary values (0/1).\n",
    "class_labels = ...\n",
    "df['class'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the first 5 rows to investigate whether they show the correct 'sex' and 'class' columns.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1 | B2 - Validate Data Frame\n",
    "Run the check functions below to check whether the `sex` and `class` columns are encoded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Send the Counter of the 'sex' column to validate if it is constructed correctly.\n",
    "module.check(\"E3_B1\", dict(Counter(df['sex'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Send the Counter of the 'class' column to validate if it is constructed correctly.\n",
    "module.check(\"E3_B2\", dict(Counter(df['class'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create two lists of column names that describe which encoding is applied.\n",
    "target_encoding_columns = ['education', 'occupation', 'native-country']\n",
    "onehot_encoding_columns = ['workclass', 'relationship', 'marital-status', 'race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Complete the function so that it return a pipeline with a 'preprocessor' and 'clf' step.\n",
    "def create_pipeline(target_enc_cols:list, onehot_enc_cols:list, scaler, clf) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Take the imputer, scaler, encoder and classifier and create and return a sklearn pipeline.\n",
    "\n",
    "    Args:\n",
    "        target_enc_cols (list): List of column names that need to be taken through Target Encoding.\n",
    "        onehot_enc_cols (list): List of column names that need to be taken through One-Hot Encoding.\n",
    "        scaler (_type_): Scaling module, used to scale the data to a set range of values.\n",
    "        clf (_type_): Classification model, which can be any model from the sklearn classification model catalog.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: Pipeline containing all preprocessing and classification models.\n",
    "    \"\"\"\n",
    "    preprocessor = make_column_transformer(\n",
    "        (..., ...),\n",
    "        (..., ...),\n",
    "        remainder=...\n",
    "    )\n",
    "    \n",
    "    return Pipeline(steps=[('preprocess', preprocessor), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create and print a pipeline to check the architecture\n",
    "pipe = create_pipeline(\n",
    "    target_enc_cols=target_encoding_columns,\n",
    "    onehot_enc_cols=onehot_encoding_columns,\n",
    "    scaler=MinMaxScaler(),\n",
    "    clf=LogisticRegression(n_jobs=-1)\n",
    ")\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## C|D. Modeling and Evaluation\n",
    "Enough of the data preprocessing, it is time to develop and train some models! We will use the library `Scikit-Learn` to do so. This library allows the user to easily switch between models, as all models have a `.fit()` and `.predict()` function. Please make sure that during initialisation (if possible) you set:\n",
    "* `n_jobs` = -1, to increase speed through `parallel computation`. <br>\n",
    "* `random_state` = 0, to fixate the end result. <br>\n",
    "\n",
    "By following the steps outlined in `Easy-LMS` we will eventually develop three models, namely: \n",
    "* `Logistic Regression`; <br>\n",
    "* `SVC`; <br>\n",
    "* `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Split our data set into independent (X) and dependent (y) variables.\n",
    "y = df['class']\n",
    "X = df.drop(['class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the first 5 rows of the X data set.\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the first 5 rows of the y data set.\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C1 - Validate Data Frame\n",
    "Run the check functions below to check whether the `independent (X)` and `dependent (y)` data sets are created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Send the shapes of both the independent (X) and dependent (y) data sets to the checker function.\n",
    "module.check(\"E3_C1\", [list(X.shape), list(y.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill the list below with all models | Make sure to add n_jobs and random_state where possible (hint: Look at documentation)\n",
    "models = [\n",
    "    ..., \n",
    "    ..., \n",
    "    ...\n",
    "    ]\n",
    "\n",
    "#TODO: Loop over list of models, create a pipeline and execute 5-fold cross validation.\n",
    "for model in models:\n",
    "    pipe = create_pipeline(\n",
    "        target_enc_cols=...,\n",
    "        onehot_enc_cols=...,\n",
    "        scaler=...,\n",
    "        clf = ...\n",
    "    )\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        estimator=..., \n",
    "        X=..., y=..., \n",
    "        cv=..., scoring='...')\n",
    "    \n",
    "    print(f'{pipe[\"clf\"].__class__.__name__} test scores: {[round(x,3) for x in cv_results[\"test_score\"]]} --> Average: {round(np.mean(cv_results[\"test_score\"]), 3)}, st.dev.: {round(np.std(cv_results[\"test_score\"]), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D1 - Validate model performances\n",
    "Run the check functions below to check whether the `model performances` are similar to ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill in the performance of the models and send it to the checker function.\n",
    "model_performances = {\n",
    "    \"LogisticRegression\": ...,\n",
    "    \"SVC\": ...,\n",
    "    \"RandomForestClassifier\": ...\n",
    "}\n",
    "\n",
    "module.check(\"E3_D1\", model_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a stratified train/test split | Please make sure to include the random_state.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D2 - Validate train/test split\n",
    "Run the check functions below to check whether the `train/test split` is created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Send the counters of both the y_train as the y_test to the checker function.\n",
    "module.check(\"E3_D2\", [dict(Counter(y_train)), dict(Counter(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill the list below with all models | Make sure to add n_jobs and random_state where possible (hint: Look at documentation)\n",
    "models = [\n",
    "    LogisticRegression(n_jobs=-1),\n",
    "    SVC(random_state=0),\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=0)\n",
    "]\n",
    "\n",
    "labels = [\"<= 50k\", \"> 50k\"]\n",
    "\n",
    "#TODO: Loop over models, train and predict and visualize results in Confusion Matrices.\n",
    "for model in models:\n",
    "    pipe = create_pipeline(\n",
    "        target_enc_cols = target_encoding_columns,\n",
    "        onehot_enc_cols = onehot_encoding_columns,\n",
    "        scaler = MinMaxScaler(),\n",
    "        clf = model\n",
    "    )\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.title(f\"{pipe['clf'].__class__.__name__}\")\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cbar=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill the list below with all models | Make sure to add n_jobs and random_state where possible (hint: Look at documentation)\n",
    "models = [\n",
    "    LogisticRegression(n_jobs=-1),\n",
    "    SVC(random_state=0),\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=0)\n",
    "]\n",
    "\n",
    "#TODO: Loop over models, train and predict and create a classification report.\n",
    "for model in models:\n",
    "    pipe = create_pipeline(\n",
    "        target_enc_cols = target_encoding_columns,\n",
    "        onehot_enc_cols = onehot_encoding_columns,\n",
    "        scaler = MinMaxScaler(),\n",
    "        clf = model\n",
    "    )\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    print(f\"-- Model: {pipe['clf'].__class__.__name__} -- \\n\")\n",
    "    print(classification_report(\n",
    "        y_true = y_test,\n",
    "        y_pred = y_pred,\n",
    "        labels = [0, 1],\n",
    "        target_names = [\"<= 50k\", \"> 50k\"]\n",
    "    ))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## E. Improve Model Performance\n",
    "To improve the model performance we first will deal with the fact that our data set is `imbalanced`. This implies that looking at both possible classes, the one is more present than the other. If this difference becomes significantly large, it might `bias` our results. For example, when a single class is present in 90% of rows, a model could reach a perfomance of 90% by only classifying this class.\n",
    "\n",
    "To fix the imbalanced data set we will use the `imblearn` library. We will apply the `RandomOverSampler` and `RandomUnderSampler`, which are the simplest way to increase or decrease the number of occurences of a given class. Please follow the steps outlined in `Easy-LMS`, which will guide you through the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before applying balancing, our counts look like: {Counter(y)}')\n",
    "\n",
    "#TODO: Apply Random Over Sampling | Please make sure random_state is set to 0.\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "print(f'After applying ROS, our counts now look like: {Counter(y_ros)}')\n",
    "\n",
    "#TODO: Apply Random Under Sampling | Please make sure random_state is set to 0.\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "print(f'After applying RUS, our counts now look like: {Counter(y_rus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set the distributions for our Cross Validated Randomized Search.\n",
    "distributions = {\n",
    "    'clf__n_estimators': np.arange(start=..., stop=..., step=..., dtype=...),\n",
    "    'clf__max_depth': list(np.arange(start=..., stop=..., step=..., dtype=...)) + [...],\n",
    "    'clf__max_features': np.arange(start=..., stop=..., step=..., dtype=...),\n",
    "    'clf__criterion': ['...','...'],\n",
    "    'clf__min_samples_leaf': np.arange(start=..., stop=..., step=..., dtype=...),\n",
    "    'clf__min_samples_split': np.arange(..., ..., step=...)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create the pipeline with Random Forest as the classifier\n",
    "pipe = create_pipeline(\n",
    "        target_enc_cols=...,\n",
    "        onehot_enc_cols=...,\n",
    "        scaler=...,\n",
    "        clf = ...(n_jobs=-1, random_state=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Execute Cross Validated Randomized Search. \n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=pipe, \n",
    "    param_distributions=distributions,\n",
    "    n_iter=100,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    random_state=0\n",
    "    ).fit(X_ros, y_ros)\n",
    "\n",
    "#TODO: Transform our results into a Pandas Data Frame.\n",
    "gridsearchResults = pd.DataFrame(\n",
    "    data={\n",
    "        'max_depth': random_search_cv.cv_results_['param_clf__max_depth'], \n",
    "        'n_estimators': random_search_cv.cv_results_['param_clf__n_estimators'], \n",
    "        'max_features': random_search_cv.cv_results_['param_clf__max_features'],\n",
    "        'criterion': random_search_cv.cv_results_['param_clf__criterion'],\n",
    "        'min_samples_leaf': random_search_cv.cv_results_['param_clf__min_samples_leaf'],\n",
    "        'min_samples_split': random_search_cv.cv_results_['param_clf__min_samples_split'],\n",
    "        'mean_test_score': random_search_cv.cv_results_['mean_test_score'],\n",
    "        'std_test_scores': random_search_cv.cv_results_['std_test_score']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearchResults.sort_values(by=\"mean_test_score\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacademy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "151409f14793981048a32979b703385565433029854af86ec7596c681e037d36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
