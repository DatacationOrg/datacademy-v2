{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment: Datacademy - Auction data\n",
    "You will be working with a real-world data set, that is collected by a `digital auction house`. <br>\n",
    "As a data professional, your task is to use this data to improve the process of the auction house. <br>\n",
    "The data consist of three related tables: `auctions`, `lots` and `bids`. <br>\n",
    "Auctions concern the actual events at which lots (items) are auctioned, after which the bids table contains all bids placed. <br>\n",
    "A more detailed overview of what these tables are comprised of is given below:\n",
    "\n",
    "1. Table `auctions` with columns:\n",
    "   * `id`: the auction id, uniquely identifying an auction (`int`).\n",
    "   * `relatedCompany`: the concerning company for which the items will be auctioned (`str`).\n",
    "   * `auctionStart`: the date and time at which the auction started (`datetime.date`).\n",
    "   * `auctionEnd`: the date and time at which the auction ended (`datetime.date`).\n",
    "   * `branchCategory`: the branch to which the product to be auctioned are categorized (`str`).\n",
    "2. Table `lots` with columns:\n",
    "   * `countryCode`: description of the country the lot is auctioned in (`str`).\n",
    "   * `saleDate`: the date and time at which the lot is sold (`datetime.date`).\n",
    "   * `auctionID`: the id reference of the auction at which the lot is offered (`int`).\n",
    "   * `lotNr`: the numeric indicator of the lot within its auction (`int`). \n",
    "   * `suffix`: additional information to the lot number (`str`).\n",
    "   * `numberOfItems`: the number of items offered within the lot (`float`).\n",
    "   * `buyerAccountID`: the id of the bidder who won the auction and bought the lot (`float`).\n",
    "   * `estimatedValue`: the estimated value of the items comprising the lot (`float`).\n",
    "   * `StartingBid`: the initial price for which the lot is offered (`float`).\n",
    "   * `reserveBid`: the minimum amount that the seller will accept as the winning bid (`int`).\n",
    "   * `currentBid`: the actual bid offered for the auctioned lot (`float`).\n",
    "   * `vat`: the percentage tax payed for the auctioned lot (`int`).\n",
    "   * `category`: the category of products to which this lot is assigned (`str`).\n",
    "   * `sold`: indicator whether the lot is sold or is left unsold (`int`).\n",
    "3. Table `bids` with columns:\n",
    "   * `auctionID`: the id reference of the auction at which the lot is offered (`int`).\n",
    "   * `lotNr`: the numeric indicator of the lot in which the bid is made (`int`).\n",
    "   * `lotID`: reference ID describing the lot in which the bid is made (`int`).\n",
    "   * `isCombination`: indicator if the bid is considered within a combination of bids (`int`).\n",
    "   * `accountID`: the id of the bidder who placed the bid (`int`).\n",
    "   * `isCompany`: indicator whether the bidder concerns a company (`int`).\n",
    "   * `bidPrice`: the price the bidder offered (`int`).\n",
    "   * `biddingDateTime`: the time the bid was placed by the bidder (`datetime.date`).\n",
    "   * `closingDateTime`: the time the lot is planned to close(`datetime.date`).\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## A. Import libraries\n",
    "\n",
    "Installation of all Python libraries is done using `poetry`, when we created the `environment` in the setup. <br>\n",
    "Below all libraries are imported and are given the correct `aliasses`, which are often used in practice. <br>\n",
    "You simply have the run the code cell and you can start creating! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import joblib\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "modulePath = \"/Modules/M10_FINAL/src\"\n",
    "dataPath = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacademy.modules import Module09\n",
    "\n",
    "module = Module09()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## B. Descriptive Analytics\n",
    "All Data Science projects start by getting a clear understanding of the data. <br>\n",
    "This section will guide you through the steps to execute descriptive analytics and familiarize yourself with the available data. <br>\n",
    "We set out some logical steps that would normally be included in such an initial data review. <br>\n",
    "These steps utilize basic Python (/Pandas) functions, used to do the follow:\n",
    "\n",
    "* `Read the data` - Read the data from the given data source.\n",
    "* `Quick view` - Look at the first x rows of the data set.\n",
    "* `Missing values` - Investigate the amount of missing values.\n",
    "* `Outliers` - Review the numerical ranges in the data set.\n",
    "* `Analyze categories` - Investigate the distribution over the available categories.\n",
    "\n",
    "Execution of these steps will give you a basic understanding of your data. <br>\n",
    "Basic understanding is sufficient for now, the rest will come when you start building and training your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Read the data\n",
    "As you will be working with larger data sets, we share them in a more suitable way. <br>\n",
    "`Parquet` is a data format designed to handle large amounts of data. <br>\n",
    "Reading them can be done in a similar way as we did with `CSV` files in the past. <br>\n",
    "However, now we use the command <code>pd.read_parquet()</code> and the file extension <code>.parquet</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Read the auctions, lots and bids using the dataPath\n",
    "auctions = pd.read_parquet(dataPath+\"/auctions.parquet\")\n",
    "lots = pd.read_parquet(dataPath+\"/lots.parquet\")\n",
    "bids = pd.read_parquet(dataPath+\"/bids.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Quick view\n",
    "To start we quickly review the first rows of the different data frames. <br>\n",
    "In doing so, we get a feeling for the structure of the data frame and the actual content that is in there. <br>\n",
    "The `.head()` function returns by default the first 5 rows. <br>\n",
    "If, for example, you want to retrieve the first 10 rows, you can use it as follows `.head(n=10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print and investigate the first 10 rows of the auctions dataframe.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print and investigate the first x rows of the lots dataframe.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print and investigate the first x rows of the bids dataframe.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions B2\n",
    "* `Q_B2_0` - What is the name of the company with `real estate` assigned as `branchCategory`, based on the top 10 rows from `auctions` dataframe? <br>\n",
    "* `Q_B2_1` - What is the value for `CurrentBid` of the lot on row index 4, from the `lots` data frame? <br>\n",
    "* `Q_B2_2` - What is the `bidPrice` for the with `AccountID` equal to `3094282` looking at the top 5 rows from the `bids` data frame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the name of the company with woodworking assigned as branchCategory, based on the top 10 rows from auctions daframe?\n",
    "Q_B2_0 = '...' \n",
    "\n",
    "module.check(\"B2_0\", Q_B2_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the value for CurrentBid of the lot on row index 4, from the lots data frame?\n",
    "Q_B2_1 = ...\n",
    "module.check(\"B2_1\", Q_B2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the bidPrice for the with AccountID equal to 3094282 looking at the top 5 rows from the bids data frame?\n",
    "Q_B2_2 = ...\n",
    "\n",
    "module.check(\"B2_2\", Q_B2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Missing Values\n",
    "Look for missing (<i>null</i>) values in the data using the `.info(show_counts=True)` command on the Pandas Dataframes. <br>\n",
    "Based on the gained insights, answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the information of the auctions dataframe\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the information of the lots dataframe\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the information of the bids dataframe\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions B3\n",
    "* `Q_B3_0` - How many null values does the `auctions` dataframe contain? <br>\n",
    "* `Q_B3_1` - Only the column `numberOfItems` has null values in the `lots` dataframe, `True` or `False`? <br>\n",
    "* `Q_B3_2` - The Bids dataframe has null values in all columns, `True` or `False`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_B3_0 = ...\n",
    "module.check(\"B3_0\", Q_B3_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_B3_1 = ...\n",
    "module.check(\"B3_1\", Q_B3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_B3_2 = ...\n",
    "module.check(\"B3_2\", Q_B3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4. Outliers\n",
    "Look for potential `outliers` in the numerical values using the `.describe()` command on the Pandas Dataframes. <br>\n",
    "Based on the gained knowledge from exercising this function, answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the description of the numerical values of the auctions Pandas Dataframe.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the description of the numerical values of the lots Pandas Dataframe.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the description of the numerical values of the bids Pandas Dataframe.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions B4\n",
    "* `Q_B4_0` - What is the maximal value for `id` in the `auctions` dataframe? <br>\n",
    "* `Q_B4_1` - What is the maximal value for `numberOfItems` in the `lots` dataframe? <br>\n",
    "* `Q_B4_2` - What is the mean value for `BidPrice` in the `bids` dataframe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_B4_0 = ...\n",
    "module.check(\"B4_0\", Q_B4_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_B4_1 = ...\n",
    "module.check(\"B4_1\", Q_B4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_B4_2 = ...\n",
    "module.check(\"B4_2\", Q_B4_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B5. Analyze categories\n",
    "Investigate the occurrence of different categories in the column `branchCategory` of the `auction` dataframe. <br>\n",
    "To do this, you will need the `Counter` function, imported from the `Collections` library. <br>\n",
    "We suggest to take a look at the documentation, which can be found by clicking <a href=\"https://docs.python.org/3/library/collections.html#collections.Counter\">here</a>. <br>\n",
    "When successfully obtained the number of occurrences per category, send this in form of a dictionary to the checker function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Retrieve the number of occurrences for all branch categories of the auctions dataframe.\n",
    "totalByCategory = ...\n",
    "\n",
    "#TODO: Place the values in a dictionary and send it to the checker function.\n",
    "Q_B5 = dict(totalByCategory)\n",
    "module.check(\"B5\", Q_B5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## C. Preprocessing and storing\n",
    "\n",
    "The initial analysis of the raw data consisted of the descriptive analysis. Now, the invention provides a method for preprocessing and storing data, which relates to the technical field of data storage and comprises the following steps: <br>\n",
    "\n",
    "* `Preprocessing` - Transform the raw data in an usefull and efficient format.\n",
    "* `Storage` - Setup a SQL database.\n",
    "* `Acquire data` - Retrieve the preprocessed data.\n",
    "* `Transmitting` - Fill the database with the data.\n",
    "* `Enhance` - Improve data consistency and quality for rigorous data management.\n",
    "\n",
    "Creation and usage of the `MySQL` database is similar to the way we used it in the `API Advanced` module. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C1. Preprocessing\n",
    "Preprocessing the data concerns the transformation of raw data into a usefull and efficient format. We already gave you some code, where your task is to extend this code by adding the requested functionality. To guide you through this process, a list of consecutive actions is shown below:\n",
    "* `Data types`: (Re-)Define the data types of the data frame columns;\n",
    "* `Outlier handling`: Remove and/or transform outliers in the data frame;\n",
    "* `Missing values`: Deal with incomplete columns in the data frame;\n",
    "* `Remove duplicate values`: Remove redundant values in the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first focus on the `data type inconsistencies`. If not done properly conflicts might occur when we will create relations between columns, as is done in a `relational database`. Based on our `Descriptive Analytics`, using the `.info()` function, we have seen the following relational columns have inconsistent data types:\n",
    "* `auctions.id` <--> `lots.auctionID`;\n",
    "* `lots.lotNr` <--> `bids.LotNr`;\n",
    "Besides also the columns `lots.numberOfItems`, `lots.buyerAccountID` and `bids.LotID` have incorrect data types. <br>\n",
    "\n",
    "Below some code is given to transform these data types in an efficient manner. The code changes values towards three different data types, namely:\n",
    "* Integers (`int`): Change id values and number of items to integer, as they do not have decimal values;\n",
    "* Datetime (`datetime.date`): Change values from object data type into datetimes, this allows easier processing of these values;\n",
    "* Boolean (`bool`): Often True/False values are depicted as an Integer 1 or 0, however it is better to write them as actual boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Run the code below to transforms each column to the correct dtype\n",
    "dtype_dic = {\n",
    "    \"integer\" : {\n",
    "        'lots': ['auctionID', 'lotNr', 'numberOfItems', 'buyerAccountID'],\n",
    "        'bids': ['LotID']\n",
    "                }, \n",
    "    \"datetime\": {\n",
    "        'auctions': ['auctionStart', 'auctionEnd'],\n",
    "        'lots': ['saleDate'],\n",
    "        'bids': ['BiddingDateTime', 'ClosingDateTime']\n",
    "                },\n",
    "    \"boolean\": {\n",
    "            'lots': ['sold'],\n",
    "            'bids': ['IsCombination', 'IsCompany']\n",
    "               }\n",
    "            }\n",
    "\n",
    "for keys_dtype in dtype_dic:\n",
    "    for keys_df in dtype_dic[keys_dtype]:\n",
    "        for column_name in dtype_dic[keys_dtype][keys_df]:\n",
    "            \n",
    "            if keys_dtype == 'integer':\n",
    "                if keys_df == 'auctions':\n",
    "                    auctions[column_name] = auctions[column_name].astype(int)\n",
    "                elif keys_df == 'lots':\n",
    "                    lots[column_name] = lots[column_name].astype(int)\n",
    "                elif keys_df == 'bids':\n",
    "                    bids[column_name] = bids[column_name].astype(int)\n",
    "\n",
    "            elif keys_dtype == 'datetime':\n",
    "                if keys_df == 'auctions':\n",
    "                    auctions[column_name] = pd.to_datetime(auctions[column_name], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "                elif keys_df == 'lots':\n",
    "                    lots[column_name] = pd.to_datetime(lots[column_name], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "                elif keys_df == 'bids':\n",
    "                    bids[column_name] = pd.to_datetime(bids[column_name], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "            \n",
    "            elif keys_dtype == 'boolean':\n",
    "                if keys_df == 'lots':\n",
    "                    lots[column_name] = lots[column_name].astype(bool)\n",
    "                elif keys_df == 'bids':\n",
    "                    bids[column_name] = bids[column_name].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ensured that our data types will not lead to any problems in the future, we can continue with handling `outliers`. Based on our Descriptive Analysis, we see no clear indication for outliers in the `auction` and `bids` data frame. However, the `lots` data frame did contain some inconsistencies in the `estimatedValue` and `currentBid` columns. These columns both contain negative values, which is of course incorrect looking at the meaning of these columns. Also, the `category` column contains 'Unknown' values, which concern missing values which need to be dealt with accordigly. Finally the `auctions` data frame contains some duplicate auctionIDs, which we need to remove in a well-thought through manner. Before transforming these columns, answer the questions below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions C1\n",
    "* `Q_C1_0` - How many rows with a negative `currentBid` does the `lots` data frame contain? <br>\n",
    "* `Q_C1_1` - How many `NaN` values has the `category` column in the `lots` data frame? <br>\n",
    "* `Q_C1_2` - How many duplicate rows does the `auctions` data frame have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Retrieve the amount of lots with a negative currentBid.\n",
    "Q_C1_0 = ...\n",
    "module.check(\"C1_0\", Q_C1_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Retrieve the amount of 'unknown' categories in the lots data frame.\n",
    "Q_C1_1 = ...\n",
    "module.check(\"C1_1\", Q_C1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Retrieve the amount of duplicate ids in the auction table.\n",
    "Q_C1_2 = ...\n",
    "module.check(\"C1_2\", Q_C1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have gained the needed knowledge through the execution of the analyses above, we can start to transform our data. First we will start with the negative `currentBid`, for which we have to decide whether to remove or replace the value. Looking at the context of the value, we should investigate if the negative currentBid has some relation to the actual final value found in the bids data frame. If not, it is best to remove the lot entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the rows of the lots data frame that contain a negative currentBid.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the bids of the first lot with a negative currentBid using the AuctionID and lotNr.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove the lots with a negative currentBid entirely from the dataframe.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regards to the `estimatedValue`, we will deal with it in an easier way. We will simply set all negative values to 0. First you have to print these values, after which you can use the `.replace({<FROM>:<TO>})` function to replace the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print all rows of the lots data frame that contain a negative estimatedValue.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Replace the value(s) with the replace() function.\n",
    "lots['estimatedValue'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, transform the `NaN` values in the category column to 'Unknown'. Pandas has a usefull function for this, named `.fillna(<VALUE>)`. Use this function below to fill the `NaN` values with the value 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Fill the missing values in the lots.category column with 'Unkown'.\n",
    "lots['category'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to find the duplicate values you have to first merge the auctions and lots data frames. The reason for this being that we work with relational data frames. As this concerns some more complex Pandas coding, we will give you the code to do this. However, we advise to analyse the code and test if you understand the way it works. We use the outer join to do this, and only do this for the relation `lots` to `auctions`. Doing this for the `lots` to `bids` relation would result in a very inefficient computational cost, looking at the size of the `bids` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the ids that do not occur in the lots auctionID column.\n",
    "remove_these_ids = pd.merge(\n",
    "    lots, auctions, \n",
    "    left_on='auctionID', \n",
    "    right_on='id', \n",
    "    how='outer')[\n",
    "        pd.merge(\n",
    "            lots, auctions, \n",
    "            left_on='auctionID', \n",
    "            right_on='id', \n",
    "            how='outer').lotNr.isna()\n",
    "            ].id.unique()\n",
    "\n",
    "# Remove these IDs\n",
    "auctions.drop(auctions.index[auctions['id'].isin(remove_these_ids)], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2. Storage\n",
    "\n",
    "Now we ave gained a greater understanding of our data and executed the needed preprocessing steps, it is time to put our data into a `production environment`. Before being able to do so, we first need to get our data to a `database` and create `APIs` to communicate with this database. For this assignment we will use a local `MySQL server`, just as we learning in the `API Advanced` module. A lot of code is aready given, however additional functionality is required to make the framework more robust. Execute the following steps:\n",
    "* `models`: Create a `model.py` file in the `app` directory, defining the correct columns and relationships;<br>\n",
    "* `schemas`: Create the `Pydantic` models to have commong attributes while creating or reading data;<br>\n",
    "* `main`: Create the `main.py` file, which represents where our API lives;<br>\n",
    "* `services`: Create the `services.py` file, which defines the requests that can be done using the API. <br>\n",
    "\n",
    "Most of these actions you already conducte in the `API - Advanced` module. Feel free to look back at your code to find hints about how to execute the creation of the files mentioned above. <br>\n",
    "\n",
    "After creating these files, you will be able to start the API and create the database:\n",
    "* `Open Terminal`: Open a terminal to run command line code; <br>\n",
    "* `Navigate`: Navigate to your locally stored folder for this module; <br>\n",
    "* `Load FastAPI`: Using the command `uvicorn app.main:app --reload`, load FastAPI and create an empty data base; <br>\n",
    "* `Open User Interface (UI)`: Open the URL on which Uvicorn is running and add `/docs` to the end. For example: `http://127.0.0.1:8000/docs`. <br>\n",
    "\n",
    "If executed correctly a new `database.db` will appear in the source directory (`src`) of this module. Now we only have to fill this database with our cleaned data. The code below can be used to send our cleaned data to the database. Note that this `uploadData` function replaces all the data curently in your database, it does not append data to it. Running the function might take some time, as the `bids` dataset is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Execute the code below to upload the data to the SQLite database.\n",
    "from app.uploadData import uploadData  \n",
    "    \n",
    "df_to_upload = {\n",
    "    'auctions': auctions,\n",
    "    'lots': lots,\n",
    "    'bids': bids\n",
    "    }\n",
    "_ = uploadData(dic_dfs=df_to_upload)                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing our data from within a MySQL database allows us to retrieve data using `SQL queries`. SQL is a `reliable` and `efficient` language used for comunicating with databases.Some advantages of using SQL are:\n",
    "* Large amount s of data are retrieved quickly and efficiently; <br>\n",
    "* For data retrieval, large number of lines of code are not required; <br>\n",
    "* It is relately easy to learn and understand. <br>\n",
    "We use the library `SQLite3` to integrate the SQLite database, which is simple-to-use and pretty strightforward. There is no need to install this modle separately, as it comes along with Python after the 2.5x version. Below some example code is given, together with an example query to retrieve all records from the `lots` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Execute the code below to execute the example query.\n",
    "conn = sqlite3.connect(f\"./database.db\")\n",
    "cur = conn.cursor()\n",
    "sql_query = \"\"\"\n",
    "            SELECT * \n",
    "            FROM 'lots'\n",
    "            LIMIT 0,1\n",
    "            \"\"\"\n",
    "display(cur.execute(sql_query).fetchall())\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions C2\n",
    "Answer the following questions by `querying` your database. Do not format the answers, but send the actual response of the query to validate if the database is initiated and the query is written properly.\n",
    "\n",
    "* `Q_C2_0` - What are the `AccountID`s of the first 5 bids in the bids table, only show AccountID? <br>\n",
    "* `Q_C2_1` - What is the highest `numberOfItems` found in the lots table, only show the MAX(numberOfItems)? <br>\n",
    "* `Q_C2_2` - How often are all `branchCategory` used within the auction table, show both branchCategory and COUNT(*)? <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(f\"./database.db\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "#TODO: Complete the query to retrieve the first 5 auctionIDs from the bids table.\n",
    "sql_query = \"\"\"\n",
    "            ...\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "Q_C2_0 = cur.execute(sql_query).fetchall()\n",
    "conn.close()\n",
    "\n",
    "module.check(\"C2_0\", Q_C2_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(f\"./database.db\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "#TODO: Complete the query to retrieve the maximum numberOfItems from the lots table.\n",
    "sql_query = \"\"\"\n",
    "            ...\n",
    "            \"\"\"\n",
    "\n",
    "Q_C2_1 = cur.execute(sql_query).fetchall()\n",
    "conn.close()\n",
    "\n",
    "module.check(\"C2_1\", Q_C2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(f\"./database.db\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "#TODO: Complete the query to retrieve the number of occurrences for all branchCategories in the auctions table.\n",
    "sql_query = \"\"\"\n",
    "            ...\n",
    "            \"\"\"\n",
    "\n",
    "Q_C2_2 = cur.execute(sql_query).fetchall()\n",
    "conn.close()\n",
    "\n",
    "module.check(\"C2_2\", Q_C2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## D. Cluster bidding behavior\n",
    "\n",
    "An important insight for the auction house is the different types of bidders. By clustering their behavior we gain insight into these different `bidding strategies`. Besides being of big descriptive value, these `clusters` can also be used in further programming. For example in `prediction models`, where the expected composition of different bidders can be of determinative value for the outcome of the auction.<br>\n",
    "\n",
    "In this section we will implement the `KMeans algorithm`, to classify certain bidders into different categories of behavior. The process of implementation is described in five consecutive steps below:\n",
    "* `Feature Engineering`: Extract usefull features from the data set; <br>\n",
    "* `Scale the data`: Apply scaling to all numerical values; <br>\n",
    "* `Find amount of clusters`: Use the so-called `elbow method` in the pre-written code below to find the optimal amount of clusters;\n",
    "* `Cluster the data`: Using the found optimal amount of clusters, cluster the entire data set; <br>\n",
    "* `Extract cluster characteristics`: Retrieve the relevant statistics of all parameters per cluster. <br>\n",
    "\n",
    "Notice that we need to query all tables in our database, let's begin to create a Pandas data frame for our database tables given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_df_from_database(table:str=None, query:str=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loading data from an SQLite database table into a Python Pandas Data Frame.\n",
    "\n",
    "    Args:\n",
    "        path_db (str): The path to the folder where the database is stored;\n",
    "        table (str): If we only want to retrieve one table, the table name is needed;\n",
    "        query (str): Write a custom query to fetch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe retrieve from the database.\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the SQLite database using the line:\n",
    "    conn = sqlite3.connect((f\"database.db\"))\n",
    "\n",
    "    # Use table or query (if both are provided use table)\n",
    "    if table:\n",
    "        query_db = f\"SELECT * FROM {table};\"\n",
    "    elif query:\n",
    "        query_db = query\n",
    "    else:\n",
    "        print(\"No input is given\")\n",
    "\n",
    "    # The line that converts SQLite data to a Panda data frame is:\n",
    "    database_table = pd.read_sql_query(query_db, conn)\n",
    "    conn.close()\n",
    "\n",
    "    return database_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D1. Feature Engineering\n",
    "First we will extract some statistics about different bidders within an auction. To do this, some characteristics of the lots are needed as well, which are:\n",
    "* `FirstBid`: Datetime object describing the moment the first bid is placed; <br>\n",
    "* `Duration`: Datetime object describing the duration of the lot (time between first and last bid). <br>\n",
    "\n",
    "These values are given through calling the function `generate_lot_statistic(data=bids)`given below. Integrate this function within the second function: `generate_bid_statistic(data)`, which is left incomplete. Follow the description in the `DOCSTRING`, which tells what the function needs to do, what it gets as input and what the output should look like. Note that generating the bid statistics might take a while, because there are a lot of computations needed as the sze of the data is rather large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lot_statistic(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate statistics regarding the Lots, describing both the datetime of the fist bid and the actual duration of the lot.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Bids data, describing the bids that are made on specific auction-lot combinations.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing the lot statistics. This needs to contain two columns:\n",
    "            - FirstBid: The datetime of the first bid placed in the lot.\n",
    "            - Duration: The time between the first bid and the scheduled time the lot is supposed to end.\n",
    "    \"\"\"\n",
    "                \n",
    "    # Group based on SaleID and LotNumber and generate characteristics\n",
    "    lot_statistic = data.groupby(['auctionID','LotNr']).agg({'BiddingDateTime': ['min'], \n",
    "                                                             'ClosingDateTime': ['max']})\n",
    "    lot_statistic.columns = ['FirstBid', 'LotEnding']\n",
    "    \n",
    "    # Convert LotEnding (Str) to timestamp object\n",
    "    lot_statistic['LotEnding'] = lot_statistic['LotEnding'].apply(pd.to_datetime, errors='coerce')\n",
    "    \n",
    "    # Calculate Lot Duration by subtracting datetime of the first bid from the lot ending datetime\n",
    "    lot_statistic['Duration'] = lot_statistic['LotEnding'] - lot_statistic['FirstBid']\n",
    "    lot_statistic['Duration'] = lot_statistic['Duration'].apply(lambda x: x/pd.Timedelta('1 minute'))\n",
    "    lot_statistic = lot_statistic.reset_index()\n",
    "    \n",
    "    return lot_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bid_statistic(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate statistics regarding the bidders, to be used to cluster different bidding behaviors.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Bids data, describing the bids that are made on specific auction-lot combinations.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing the statistics regarding bidders. This needs to contain nine columns:\n",
    "            - AuctionID: The ID reference of the auction in which the bid was placed.\n",
    "            - LotNr: The number reference of the lot in which the bid was placed.\n",
    "            - AccountID: The ID reference of the bidder which placed the bid.\n",
    "            - NOB: The Number Of Bids this bidders has placed in the concerning auction lot.\n",
    "            - ABP: The Average Bid Price of all the bids that the bidders has placed in the concerning auction lot.\n",
    "            - HBP: The Highest Bid Price of all the bids that the bidders has placed in the concerning auction lot.\n",
    "            - TOE: The Time Of Entry, describing at what percentage of total lot duration the bidder placed his first bid.\n",
    "            - TOX: The Time Of Exit, describing at what percentage of total lot duration the bidder placed his last bid.\n",
    "    \"\"\"\n",
    "    # Ensure datetime type in datetime variables\n",
    "    data['BiddingDateTime'] = pd.to_datetime(data['BiddingDateTime'], format='ISO8601')\n",
    "    data['ClosingDateTime'] = pd.to_datetime(data['ClosingDateTime'], format='ISO8601')\n",
    "\n",
    "    # Group based on accountID, LotNr and AccountID and generate NOB, ABP, HBP, TOE and TOX.\n",
    "    bid_statistic = data.groupby(['auctionID','LotNr','AccountID']).agg({'BidPrice': ['count', 'mean', 'max'], \n",
    "                                                                         'BiddingDateTime': ['min', 'max']})\n",
    "    bid_statistic.columns = ['NOB', 'ABP', 'HBP', 'TOE', 'TOX']\n",
    "    bid_statistic = bid_statistic.reset_index()\n",
    "\n",
    "    # Generate statistics of the lot and merge with bid statistic\n",
    "    lot_statistic = generate_lot_statistic(data)\n",
    "    bid_statistic = pd.merge(bid_statistic, lot_statistic, on=['auctionID', 'LotNr'], how='left')\n",
    "\n",
    "    # Convert static time characteristics (TOE and TOX) to relative time characteristics using the merged Lot statistics (FirstBid and Duration)\n",
    "    bid_statistic['TOE'] = (bid_statistic['TOE'] - bid_statistic['FirstBid']).apply(lambda x: x/pd.Timedelta('1 minute'))\n",
    "    bid_statistic['TOE'] = bid_statistic['TOE'] / bid_statistic['Duration']\n",
    "    bid_statistic['TOX'] = (bid_statistic['TOX'] - bid_statistic['FirstBid']).apply(lambda x: x/pd.Timedelta('1 minute'))\n",
    "    bid_statistic['TOX'] = bid_statistic['TOX'] / bid_statistic['Duration']\n",
    "    \n",
    "    # Set outliers in normalized TOE or TOX (> 1) to 1\n",
    "    bid_statistic['TOE'] = [x if x <= 1 else 1 for x in bid_statistic['TOE']]\n",
    "    bid_statistic['TOX'] = [x if x <= 1 else 1 for x in bid_statistic['TOX']]\n",
    "\n",
    "    # Remove lot statistics from the data again\n",
    "    bid_statistic.drop(['FirstBid', 'LotEnding'], axis=1, inplace=True)\n",
    "\n",
    "    return bid_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the padas_df_from_database() function to retrieve the bids data\n",
    "bids_from_db = ...\n",
    "\n",
    "#TODO: Use the create_bid_statistic() function to create the bid statistics \n",
    "bid_statistic = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2. Scale the data\n",
    "\n",
    "To prevent the different distributions in the data to interfere with the model results, we will scale the data. To keep things simple, we will apply `Min-Max Scaling`. This method is the most simple, but also the most applied method out there. It will simply spread all values over a range from 0 to 1, as explained in the library [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data: pd.DataFrame) -> np.array:\n",
    "    \"\"\"\n",
    "    Apply min-max scaling to the data to have all values between 0 and 1.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Bid statistics dataframe, containing all characteristic values of bidders in a given auction lot.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Normalized bid statistic data, ready to be used for clustering.\n",
    "    \"\"\"\n",
    "    # Only select characteristic values, namely NOB, ABP, HBP, Duration (excluding TOE and TOX as they are already scaled)\n",
    "    bid_stat_array = bid_statistic.iloc[:,3:8]\n",
    "    bid_stat_array.drop(['TOE', 'TOX'], axis=1, inplace=True)\n",
    "\n",
    "    # Initialize minmax scaler and apply to selected data\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    norm_bid_stat = min_max_scaler.fit_transform(bid_stat_array)\n",
    "\n",
    "    # Reappend the TOE and TOX values (as they were excluded upon selection)\n",
    "    norm_bid_stat = np.c_[norm_bid_stat, bid_statistic['TOE'], bid_statistic['TOX']]\n",
    "\n",
    "    return norm_bid_stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = ...\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3. Find amount of clusters\n",
    "\n",
    "A fundamental setp for any clustering algorithm is the determination of the optimal number of clusters into which the data needs to be clustered.However, in most cases, the optimal amount of clusters is not known beforehand. To decide the amount of clusters that is optimal with regards to the data, the Elbow Method will be used. For more information about the Elbow Method, [Click Here!](https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(scaled_data: np.array, start_: int, end_: int) -> None:\n",
    "    \"\"\"Code to generate a visual representation of the elbow method over the given range.\n",
    "\n",
    "    Args:\n",
    "        scaled_data (np.array): data to be used for clustering.\n",
    "        start_ (int): Starting value for the amount of clusters.\n",
    "        end_ (int): Ending value for the amount of clusters.\n",
    "    \"\"\"\n",
    "    nr_of_clusters = list(range(start_, end_))\n",
    "    within_cluster_variation = []\n",
    "    \n",
    "    #TODO: Loop over the nr_of_clusters, fit a KMeans algorithm and append the WCV to the created list.\n",
    "    ...\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(nr_of_clusters, within_cluster_variation, color='red')\n",
    "    plt.xticks(nr_of_clusters)\n",
    "    plt.ylabel('Within Cluster Variation (WCV)')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.title('Elbow method')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: First complete the function above and then execute the Elbow method to visualize the cluster performances.\n",
    "elbow_method(scaled_data=scaled_data, start_=2, end_=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions D3\n",
    "* `Q_D3` - What is the optimal amount of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Read the Elbow plot above and decide the optimal amount of clusters.\n",
    "optimal_amount_of_clusters = ...\n",
    "\n",
    "Q_D3 = optimal_amount_of_clusters\n",
    "module.check(\"D3\", Q_D3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D4. Cluster the data\n",
    "\n",
    "After finding the optimal amount of clusters, we only have to run the `KMeans` algorithm once more to cluster the data. Thereafter, we will combine the data with the `clustering labels` into a single Pandas data frame. This final data frame we will use to further investigate the different `bidding behaviours`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the KMeans algorithm to cluster the data.\n",
    "kmeans = KMeans(\n",
    "    init= 'k-means++', \n",
    "    n_clusters=optimal_amount_of_clusters, \n",
    "    random_state=0,\n",
    "    n_init=10\n",
    "    ).fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the clustering labels to the bid statistics.\n",
    "bid_statistic['cluster'] = kmeans.labels_\n",
    "bid_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions D4\n",
    "* `Q_D4` - What are the amount of rows assigned to all clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Retrieve the counts per cluster\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill in the values of the dictionary below\n",
    "Q_D4 = {\n",
    "    \"0\":...,\n",
    "    \"1\":...,\n",
    "    \"2\":...\n",
    "}\n",
    "\n",
    "module.check(\"D4\", Q_D4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D5. Extract cluster characteristics\n",
    "\n",
    "Solemnly having the data clustered does not really have any `descriptive business value`. What we want to see is what these clusters imply, which is done through extraction of `cluster characteristics`. In a similar manner as we extracted the clustering characteristics, we can apply Pandas `Groupby` to extract these values. What we would like to see is the following average values for all clusters separately:\n",
    "* `NOB` - Average Number Of Bids; <br>\n",
    "* `ABP` - Average Bid Price; <br>\n",
    "* `HBP` - Average Highest Bid Price; <br>\n",
    "* `TOE` - Average Time Of Entry; <br>\n",
    "* `TOX` - Average Time Of Exit. <br>\n",
    "\n",
    "Use the Pandas `.groupby().agg()` function to extract these features from the bid statistics. Make sure to groupby the `cluster` column, to ensure that the charateristics are calculated for every distinct cluster separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Apply the .groupby().agg({}) function to extract the cluster characteristics.\n",
    "cluster_statistics = bid_statistic.groupby(['cluster']).agg({\n",
    "    'NOB': ['mean'], \n",
    "    'ABP': ['mean'], \n",
    "    'HBP': ['mean'], \n",
    "    'TOE': ['mean'], \n",
    "    'TOX': ['mean']\n",
    "    })\n",
    "\n",
    "cluster_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions D5\n",
    "To answer the questions below, we will ask you to return a complete row of a data frame. This can be done using the `.iloc[row_idx, :]` functionality. Make sure to send in a list by adding `.tolist()` to the end of your iloc statement.\n",
    "\n",
    "* `Q_D5_0` - What are the statistics of the bidder that only places bids at the beginning of an auction? <br>\n",
    "* `Q_D5_1` - What are the statistics of the bidder that places bids for the largest fraction of time a lot is being auctioned? <br>\n",
    "* `Q_D5_2` - What are the statistics of the bidder that only places bids at the end of an auction? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Extract the statistics of the early bidder\n",
    "Q_D5_0 = ...\n",
    "\n",
    "module.check(\"D5_0\", Q_D5_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Extract the statistics of the full duration bidder\n",
    "Q_D5_1 = ...\n",
    "\n",
    "module.check(\"D5_1\", Q_D5_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Extract the statistics of the late bidder\n",
    "Q_D5_2 = ...\n",
    "\n",
    "module.check(\"D5_2\", Q_D5_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## E. Predict sale or no-sale\n",
    "After clustering (which is unsupervised learning) we are now going to apply supervised learning in the form of forecasting. To improve the auction platform we want to automate the decision about what the starting bid should be. For this, we will use a prediction model which predicts if a product will be sold. By increasing the starting bid iteratively and continuously checking if a product will be sold, we can define a breaking point. This breaking point is where the product goes from a sold prediction to an unsold prediction, which shows the highest starting bid which will result in a sell. <br>\n",
    "\n",
    "This use case is of course greatly simplified, as the assumption that a higher starting bid will result in a higher final bid is quite a big one. But for the sole purpose of testing whether you understand the fundamentals of supervised learning it will suffice. What we want you to do is the following:\n",
    "* `Merge & Preprocess`: Manually prepare auctions and lots data set and build a `preprocessing pipeline`; <br>\n",
    "* `Train/Test split`: Split the lots data set into a 70/30 train/test split <b>(with `random_state=0`!)</b>; <br>\n",
    "* `Initialize models`: Initialize both the `LogisticRegression` and `RandomForestClassifer` using defaul settings and `random_state=0`; <br>\n",
    "* `Train models`: Train both models on the train set using `.fit(X, y)`; <br>\n",
    "* `Predict`: Use the trained model to predict the labels of the test set using `.predict(X, y)`; <br>\n",
    "* `Evaluate models`: Evaluate the performance of the models using the `accuracy` and plot the `confusion matrix`; <br>\n",
    "* `Save`: Save the best performing model to a `model.pkl` file for later usage; <br>\n",
    "* `Implement`: Integrate the model into `API` to define a starting bid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1. Merge & Preprocess\n",
    "\n",
    "First we will merge the auctions and lots table using a SQL query, as this is computationally most efficient. Thereafter, two transformations are neede to be done, namely:\n",
    "* `Datetime values` - Transform datetime objects into tangible numeric value; <br>\n",
    "* `Categorical values` - Apply one-hot encoding to categorical variables. <br>\n",
    "\n",
    "After completing these manual preprocessing steps we will create a `preprocessing pipeline`. This enables us to more quickly test out different preprocessing steps, while ensuring that we execute all the steps in the correct order. This prevents the previously explained phenomenon of `data spoilage` to occur. <br>\n",
    "\n",
    "Throughout the steps we will ask you to send in the dimensions of the data frame, by submitting the output of the `.shape` functionality. We transform the Tuple into a list, as our checker function is build to only deal with lists. You do not have to write the code, when you named the data frame correctly then simply run the code and your data frame will be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write a SQL query with a left join of auctions on lots.auctionID = auctions.id\n",
    "#TODO: SELECT auctionID, lotNr, numberOfItems, estimatedValue, startingBid, reserveBid and sold from the lots table\n",
    "#TODO: SELECT auctionStart, auctionEnd and branchCategory from the auctions table\n",
    "sql_merge_query = \"\"\"\n",
    "                ...\n",
    "                  \"\"\"\n",
    "\n",
    "Lots_Sales_Pred = pandas_df_from_database(query=sql_merge_query)\n",
    "Lots_Sales_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Run the code below to check whether your query resulted in the correct output.\n",
    "Q_E1 = list(Lots_Sales_Pred.shape)\n",
    "\n",
    "module.check(\"E1\", Q_E1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform auctionStart and auctionEnd into a single column depicting the duration of the auction in hours\n",
    "Lots_Sales_Pred[['auctionStart', 'auctionEnd']] = Lots_Sales_Pred[['auctionStart', 'auctionEnd']].apply(pd.to_datetime, errors='coerce') \n",
    "Lots_Sales_Pred['auctionDuration'] = (Lots_Sales_Pred['auctionEnd'] - Lots_Sales_Pred['auctionStart']).apply(lambda x: abs(x/pd.Timedelta('1 hour')))\n",
    "Lots_Sales_Pred.drop(['auctionStart', 'auctionEnd'], axis=1, inplace=True)\n",
    "\n",
    "Lots_Sales_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill in the blanks below to complete the pipeline creation function.\n",
    "def create_pipeline(scaler, encoder, clf, cat_col:list, num_col:list) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Take the imputer, scaler, encoder and classifier and create and return a sklearn pipeline.\n",
    "\n",
    "    Args:\n",
    "        scaler (_type_): Scaling module, used to scale the data to a set range of values.\n",
    "        encoder (_type_): Encoding module, used to transform categorical values to a workable format.\n",
    "        clf (_type_): Classification model, which can be any model from the sklearn classification model catalog.\n",
    "        cat_col (list): A list of the categorical columns that need to be transformed.\n",
    "        num_col (list): A list of the numerical columns that need to be transformed.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: Pipeline containing all preprocessing and classification models.\n",
    "    \"\"\"\n",
    "    # 2 sub-pipelines, one for numeric features, other for categorical ones\n",
    "    numeric_pipe = ...\n",
    "    categorical_pipe = ...\n",
    "\n",
    "    # Using categorical pipe for feature State, numeric pipe otherwise\n",
    "    preprocessor = make_column_transformer((..., ...), \n",
    "                                            (..., ...),\n",
    "                                            remainder='passthrough')\n",
    "    \n",
    "    return Pipeline(steps=[('preprocess', ...), ('clf', ...)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2. Train/Test Split\n",
    "\n",
    "The quality of the `supervised learning` method is in a big way dependent on de amount of data that is available. As we have to split the data into a `train and test set`, the set used for training the model also decreases in size. For this reason it is a must to put this data to good use to get a reliable evaluation of the performance of different models. <br>\n",
    "\n",
    "Using the `train_test_split()` function we will create a 70/30 train/test split. Please make sure to set the `random_state` to 0, to retrieve similar answers everytime you run the code. This is needed to help our `evaluation functions` to work properly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split the data in independent (X) and dependent (y) variables\n",
    "X_sale = Lots_Sales_Pred.copy().drop(['auctionID', 'lotNr', 'sold'], axis=1)\n",
    "y_sale = Lots_Sales_Pred['sold'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ...\n",
    "print(f'Created a training set containing {len(X_train)} records and a test set containing {len(X_test)} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Submit the shape of the data frames.\n",
    "Q_E2 = [list(X_train.shape), list(X_test.shape), list(y_train.shape), list(y_test.shape)]\n",
    "\n",
    "module.check(\"E2\", Q_E2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3. Initialize models\n",
    "Before training and using our models, we first have to `initialize` them. For this we will use the Python library `Scikit-Learn`. Please initialize the following models:\n",
    "* `MinMaxScaler()` - Used to scale the numerical data between the values of 0 and 1;\n",
    "* `OneHotEncoder()` - Used to encode the categorical data to either a 1 or 0;\n",
    "* `LogisticRegression()` - First model to test to classify either a sale or no-sale (set `max_iter` to 5000 and `random_state` to 0);\n",
    "* `RandomForestClassifier()` - First model to test to classify either a sale or no-sale (set `random_state` to 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the scaler and encoder\n",
    "scaler = MinMaxScaler()\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Define columns to pass through scaler and encoder\n",
    "sclr_col = ['numberOfItems', 'estimatedValue', 'startingBid', 'reserveBid', 'auctionDuration']\n",
    "encd_col = ['branchCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize both models.\n",
    "classifiers = [\n",
    "     ...,\n",
    "     ...\n",
    "     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E4. Train models and predict\n",
    "Now we have initialized our models, it is time to train them on our data. Training can be done using the `.fit(X,y)` function that is an inherent function of every `scikit-learn` model. Make sure to train the model only on the `train set`, as we will use the `test set` for evaluation of the trained model. Training will be done using a `pipeline`, which ensures that all the preprocessing steps are done in the correct order. To create such a pipeline we utilize the `create_pipeline()` function we developed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a dictionary to hold the classification results.\n",
    "predictions_clf = {\n",
    "    'LogisticRegression': None,\n",
    "    'RandomForestClassifier': None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Loop over both classifiers, create and fit a pipeline and retrieve the predictions.\n",
    "for clf in classifiers:\n",
    "    name_clf = str(clf).split('(')[0]\n",
    "\n",
    "    # Combine with learning algorithm in another pipeline\n",
    "    pipe = ...\n",
    "    fitted_pipe = ...\n",
    "\n",
    "    # Save the predicted values in a list\n",
    "    predictions_clf[name_clf] = ...\n",
    "    \n",
    "    # Print the test accuracy\n",
    "    print(f\"{name_clf} test accuracy: {fitted_pipe.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and print the Confusion Matrix for the Logistic Regression.\n",
    "print(f'Confusion Matrix: Logistic Regression')\n",
    "cm_logRes = confusion_matrix(y_test, predictions_clf['LogisticRegression'])\n",
    "disp = ConfusionMatrixDisplay(cm_logRes, display_labels=['Sold', 'Unsold']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Transform code to print the Confusion Matrix for the Random Forest Classifier.\n",
    "print(f'Confusion Matrix: Random Forest Classifier')\n",
    "cm_rf = confusion_matrix(y_test, predictions_clf['RandomForestClassifier'])\n",
    "disp = ConfusionMatrixDisplay(cm_rf, display_labels=['Sold', 'Unsold']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E5. Save model\n",
    "Now we have trained and tested our models, we are able to define which model performs best and thus which model we want to further investigate and/or put into `operation`. Now we will save the best performing model, but before we do we will train a new version on all our data. No train/test set are needed anymore, as we will not test the model against other models. We will solemnly use it, so we can utilize the entire data set to further optimize the performance. <br>\n",
    "\n",
    "To save your model we will use a `pickle (.pkl)` file, which is often used to save objects in Python. We will do this using the `joblib.dump()` function. Later on we can load the .pkl file and retrieve the trained version of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Develop and train a new pipeline with the best performing model as classifier:\n",
    "pipe_to_save = create_pipeline(\n",
    "                    scaler=MinMaxScaler(),\n",
    "                    encoder=OneHotEncoder(),\n",
    "                    clf=RandomForestClassifier(random_state=0),\n",
    "                    cat_col=['branchCategory'],\n",
    "                    num_col=['numberOfItems', 'estimatedValue', 'startingBid', 'reserveBid', 'auctionDuration']\n",
    "                    )\n",
    "\n",
    "trained_pipe = pipe_to_save.fit(X=X_sale, y=y_sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use joblib.dump() to save the model with the name \"trained_pipeline.pkl\".\n",
    "joblib.dump(trained_pipe, f\"trained_pipeline.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E6. Implement\n",
    "To put our model to use, we will integrate it into the `API` which enables the user to create a lot. When a new lot is created a fundamental step is to decide the `starting price` of the auctioned lot. To make this data driven, your task is to implement the prediction model to automatically set an optimal price. This is done through `incrementally` increasing the starting bid until the lot prediction of the lot goes from sold to unsold. <br>\n",
    "\n",
    "Implementing the automation of the starting bid is of course hugely `simplified`, as it acquiesces on the assumption that a higher starting bid will result in a higher final bid. However, for the sole purpose of deploying an AI model it suffices. <br>\n",
    "\n",
    "To execute this assignment, go to the directory called `app` and open the python file `Services`. Here you need to change the functionality of the `create_lot()` function. Within this function some basis code is already given (which you have to uncomment), to get you up to speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions E6\n",
    "* `Q_E6` - What is the starting price of the following auction:\n",
    "\n",
    "Using your newly created API in the Swagger UI and test the following auction:\n",
    "* `numberOfItems`: 7; <br>\n",
    "* `estimatedValue`: 100; <br>\n",
    "* `reserveBid`: 1.0; <br>\n",
    "* `branchCategory`: transport; <br>\n",
    "* `auctionDuration`: 250.0; <br>\n",
    "* `min_starting_bid`: 5; <br>\n",
    "* `max_starting_bid`: 155; <br>\n",
    "* `step_size`: 5. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_E6 = ...\n",
    "module.check(\"Q_E6\", Q_E6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Dashboarding\n",
    "Finally we will guide you through the process of connecting your database to a PowerBI dashboard. In most cases, this is a logical step in a Data Science project. Executing the steps below will allow you to start creating a dashboard to gain data insights.\n",
    "* `Setup environment` - Download the `SQLite ODBC driver` and `PowerBI` (if not done already); <br>\n",
    "* `Get Data` - Open PowerBI and press the \"Get Data\" button and make an ODBC connection; <br>\n",
    "* `Settings` - Set DSN to None and open \"Advanced settings\"; <br>\n",
    "* `Connection string` - Write the connection string as follows: `DRIVER={SQLite3 ODBC Driver};Database=C:\\ [PATH TO THIS DIRECTORY] \\database.db`; <br>\n",
    "* `Login` - Write a random username and password (will not be checked but needed to pursue); <Br>\n",
    "* `Load data` - Press \"Load Data\" to get the data into `PowerBI`; <br>\n",
    "* `Bug Fixing` - Fix `many-to-many` cardinality by creating additional column with `auctionLotID` in both lots and bids data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4079134c0179285e3a55d2e425b94f441ed12cd55a998c83f6f5077fa904635"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
